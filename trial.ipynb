{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROMPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferWindowMemory(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m CTransformers(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mllama-2-7b-chat.ggmlv3.q4_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m                     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m                     config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m512\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9\u001b[39m})\n\u001b[1;32m---> 10\u001b[0m chain_type_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mPROMPT\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory\u001b[39m\u001b[38;5;124m\"\u001b[39m: ConversationBufferWindowMemory(memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[0;32m     11\u001b[0m question_answer_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m     12\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     13\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PROMPT' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Setup manually (similar to app.py)\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "llm = CTransformers(model=\".\\model\\llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                    model_type=\"llama\",\n",
    "                    config={'max_new_tokens': 512, 'temperature': 0.9})\n",
    "chain_type_kwargs={\"prompt\": PROMPT, \"memory\": ConversationBufferWindowMemory(memory_key=\"history\", input_key=\"question\")}\n",
    "question_answer_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={'k': 2}),\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test\n",
    "result = question_answer_chain.invoke({\"query\": \"What is a business model?\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFDirectoryLoader(\"data\")\n",
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\" \\n  Page | 1 \\n \\nE-commerce - Business models \\nEvery business operates on business model of its own. Business model defines how a company \\nruns its operations & generates revenue. Every viable organization is built on a sound business \\nmodel. Selecting an ecommerce business model is a challenge, especially for beginners who \\nhave little to no experience in the industry. If a business model is successfully executed, an \\necommerce venture can become a significant source of income. Let us go through some most \\nprevalent e-commerce business models in detail: \\n1. B2B Business model \\nA website following the B2B business model sells its products to an intermediate buyer who \\nthen sells the product to the final customer. As an example, a wholesaler places an order from a \\ncompany's website and after receiving the consignment, sells the end product to the final \\ncustomer who comes to buy the product at one of its retail outlets.  \\n \\n \\n  \\n \\n \\n \\nExample \\n      \\nIndiamart is one of the best examples of B2B e-commerce business. Founded in 1999, the \\ncompany’s mission is ‘ to make doing business easy ’. It is India’s largest B2B marketplace. \\nWith 60% market share of the online B2B Classified space in India, the channel focuses on \\nproviding a platform to Small & Medium Enterprises (SMEs), Large enterprises as well \\nas individuals.  \\n \\n2. B2C Business model \\nA website following B2C business model sells its products directly to a customer, the end user. A \\ncustomer can choose the products shown on the website and place an order for them. They may \\nuse banking channels or payment on delivery options.  \\nThere are two broad categories under this model – Direct selling and market place model. \\nI. Direct selling  refers to the manufacturer or wholesaler of the product/brand selling \\ntheir goods directly to the customers on their own website.  \\nBusiness sells goods \\nor provides services  \\nOnline Portal  \\nThrough  \\nAnother Business\\nTo \", metadata={'source': 'data\\\\E-commerce_Business_Models (1).pdf', 'page': 0}),\n",
       " Document(page_content=' \\n  Page | 2 \\n \\n \\n \\n \\nII. Market place model  refers to an intermediary website where several sellers list \\ntheir products. The businesses use this as their market place instead of having their \\nown website. The customers may by a wide range of heterogeneous products at \\ncomparative rates under this model. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nExamples \\nFlipkart \\nFlipkart started off with a direct-to-consumer model selling books and some other products, \\nbefore turning to a marketplace model which connect sellers and buyers and expanding its \\nVarious business and service \\nproviders list themselves on the E \\ncommerce platform advertising \\ntheir services/goods. \\nThe E commerce site verifies \\nthe orders placed and \\ndespatches goods from its \\nwarehouse/supplier’s \\nwarehouse/ connects the \\ncustomer to the relevant \\nprofessional. \\nThe E commerce site \\nverifies the businesses \\nwho intend to be listed \\nand advertises their \\noffering on their website. \\nThe payment is received \\nby the E commerce site, \\ncharges a commission for \\nthe online market place \\nand pays the supplier for \\nhis product. ', metadata={'source': 'data\\\\E-commerce_Business_Models (1).pdf', 'page': 1}),\n",
       " Document(page_content=' \\n  Page | 3 \\n \\ncatalogue. The sources of income to Flipkart include seller commission, advertisements, \\nlogistics and convenience fees.  \\nJust dial \\nJustdial is India’s search engine for local search market which initially started as a classified \\nwebsite but soon transformed into a local search engine. They used word-of-mouth strategy to \\nadvertise themselves. Focussing on local brands and small businesses, the company became \\nfamous among the masses.  \\nUrban Clap \\nUrban Clap is a company that provides a variety of services of professionals and blue collar \\nworkers at the convenience of customers’ home. Once can hire electricians, yoga trainers, \\nlawyers, engineers, chartered accountants, beauticians, photographers, interior designers, etc. \\n \\n3. C2C business model \\nA website following the C2C business model helps consumers to sell their assets by publishing \\ntheir information on the website. Website may or may not charge the consumer for its services. \\nAnother consumer may opt to buy the product of the first customer by viewing the \\npost/advertisement on the website. \\n \\n \\n \\nExamples \\n  \\nEbay is a website which caters both B2C and C2C transactions. In C2C e-bay has a varied feature \\nof auction where buyers bid for the product. Such feature helps both the buyers and sellers to \\nsell and buy the used goods at their best and reasonable prices. E- bay is the most suitable \\nexample for C2C transactions. \\n \\n \\n  \\nOLX is another best example for C2C e-commerce business model which works according to \\nabove mentioned process but does not involves auctions. Rather they collect nominal fees to list \\nthe sellers’ product in the home page to attract the prospective buyer of that product. These \\nwebsites even rate the sellers and buyers on basis of their past transactions, from which a new \\nbuyer can easily identify the genuine sellers. \\n \\nC2C Model : How it works? \\n', metadata={'source': 'data\\\\E-commerce_Business_Models (1).pdf', 'page': 2}),\n",
       " Document(page_content=' \\n  Page | 4 \\n \\n \\n \\n \\n \\njdj \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n4. C2B business model \\nIn this model, a consumer approaches a website showing multiple business organizations for a \\nparticular service. The consumer places an estimate of amount he/she wants to spend for a \\nparticular service.  \\n \\nExamples \\nFreelancer.com  \\nFreelancer is an Australian crowd sourcing marketplace website, which allows potential \\nemployers to post jobs that freelancers can then bid to complete. Where the freelancers are \\nmostly individuals who provides personalized jobs to the business organisations for some \\nconsideration. This is one of the best examples for C2B e-commerce business model. \\nProduct listing  \\nSellers who intend to sell their \\nproducts need to list their products \\nin specific categories which help the \\ne-commerce platform to advertise \\nthe same for its prospective buyers \\nof the product \\nBuyers \\nAfter product listing the e-\\ncommerce platforms display \\nthe advertisement and buyers \\ncan easily find the items for \\nsale with the product and \\nseller description along with \\nthe pictures of the products. \\nNegotiation  \\nWhen there are many buyers \\nwho are interested in the \\nproduct the seller either may \\ngo for auction or the buyers \\nmay contact the sellers and \\nnegotiate to its best.  \\nPayment \\nAfter the buyer is finalized, the \\npayment by the buyer through \\nany payment platform except \\ncredit cards and debit cards. And \\nfurther the marketplace offered \\nby the companies such as e-bay \\nand Olx even charge some % of \\nsale amount as their commission \\nfrom the sellers. \\nShipping/Delivery  \\nAfter the payment process either \\nthe buyer or the seller shall initiate \\nthe delivery or shipping .If seller \\ninitiates the delivery then delivery \\ncharges has to be borne by the \\nbuyers and the responsibility of the \\nproducts delivered shall remain with \\nthe buyer even during the course of \\ntransit. \\nSeller \\nSellers who are interested in \\nselling the products (mostly used \\nproducts) can approach to E-\\ncommerce platforms for listing \\ntheir products for sale. (E-bay & \\nOlx)  \\n  ', metadata={'source': 'data\\\\E-commerce_Business_Models (1).pdf', 'page': 3}),\n",
       " Document(page_content=' \\n  Page | 5 \\n \\nUpwork.com \\n Another best example for this model is Upwork.com. Through Upwork, businesses get more \\ndone, connecting with proven professionals to work on projects from web and mobile app \\ndevelopment to SEO, social media marketing, content writing, graphic design, admin help and \\nthousands of other projects. Upwork makes it fast, simple, and cost-effective to find, hire, work \\nwith, and pay the best professionals anywhere, any time. \\n \\n5. G2C Business model \\n \\nGovernments use G2C model websites to approach citizen in general. Such websites \\nsupport auctions of vehicles, machinery, or any other material. Such website also provides \\nservices like registration for birth, marriage or death certificates. The main objective of G2C \\nwebsites is to reduce the average time for fulfilling citizen’s requests for various government \\nservices \\n6. G2G models \\n \\nGovernment to government (G2G) is the electronic sharing of data and/or information systems \\nbetween government agencies, departments or organizations. The goal of G2G is to support e-\\ngovernment initiatives by improving communication, data access and data sharing. When the \\nexchange of information and services is within the periphery of the government, is termed as \\nG2G interaction. This can be both horizontal, i.e. among various government entities and \\nvertical, i.e. between national, state and local government entities and within different levels of \\nthe entity. \\n \\n7. C2G Model \\nThis model indicates the transactions between government and consumers. Where consumer \\nprovides any services or feedback that adds value to the government administration or \\nauthorities. It is an electronic platform where consumers interact with the government.  It \\ncovers the areas such as election, votes, and taxation.   \\nThe C2G, short for Consumer to Government E-commerce business allows consumers to provide \\nfeedback or ask for information about government authority from the public sector. When you \\npay an electricity bill via the government website, it is a favourite E-commerce business model. \\nHence, the C2G model of business allows consumers to reach higher authorities without going \\naround in circles. \\nExample: \\n A consumer can pay his income tax or GST online. The transaction involved in this case are C2G \\ntransactions. \\n \\nContributions made by: Mohammed Arshaq, Gouthami R, Srijit Nair, Sushma \\nSources:https://www.clarity-ventures.com/articles/what-is-consumer-to-consumer-ecommerce  \\nhttps://www.ideamotive.co/blog/marketplaces-business-model-overview \\nhttps://onlineseoblog.com/startup-business-model-analysis/b2b-b2c-startups-working-operation/meesho-reselling-works-\\nrevenue-interview/  \\nhttps://jungleworks.com/online-marketplace-ebay/  \\nhttps://www.apptunix.com/blog/how-does-ebay-work/ \\nhttp://etenacious.com/the-six-types-of-e-commerce-business/ ', metadata={'source': 'data\\\\E-commerce_Business_Models (1).pdf', 'page': 4})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pinecone_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(\"chatbot-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"chatbot-2\"\n",
    "for i, t in zip(range(len(text_chunks)), text_chunks):\n",
    "   query_result = embeddings.embed_query(t.page_content)\n",
    "   index.upsert(\n",
    "   vectors=[\n",
    "        {\n",
    "            \"id\": str(i),  # Convert i to a string\n",
    "            \"values\": query_result, \n",
    "            \"metadata\": {\"text\":str(text_chunks[i].page_content)} # meta data as dic\n",
    "        }\n",
    "    ],\n",
    "    namespace=\"real\")\n",
    "   index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 0.3.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = 'text'\n",
    "vectorstore = Pinecone(\n",
    "    index, embeddings, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '0',\n",
      "              'metadata': {'text': 'Page | 1 \\n'\n",
      "                                   ' \\n'\n",
      "                                   'E-commerce - Business models \\n'\n",
      "                                   'Every business operates on business model '\n",
      "                                   'of its own. Business model defines how a '\n",
      "                                   'company \\n'\n",
      "                                   'runs its operations & generates revenue. '\n",
      "                                   'Every viable organization is built on a '\n",
      "                                   'sound business \\n'\n",
      "                                   'model. Selecting an ecommerce business '\n",
      "                                   'model is a challenge, especially for '\n",
      "                                   'beginners who \\n'\n",
      "                                   'have little to no experience in the '\n",
      "                                   'industry. If a business model is '\n",
      "                                   'successfully executed, an'},\n",
      "              'score': 0.777944088,\n",
      "              'values': []},\n",
      "             {'id': '3',\n",
      "              'metadata': {'text': 'as individuals.  \\n'\n",
      "                                   ' \\n'\n",
      "                                   '2. B2C Business model \\n'\n",
      "                                   'A website following B2C business model '\n",
      "                                   'sells its products directly to a customer, '\n",
      "                                   'the end user. A \\n'\n",
      "                                   'customer can choose the products shown on '\n",
      "                                   'the website and place an order for them. '\n",
      "                                   'They may \\n'\n",
      "                                   'use banking channels or payment on '\n",
      "                                   'delivery options.  \\n'\n",
      "                                   'There are two broad categories under this '\n",
      "                                   'model – Direct selling and market place '\n",
      "                                   'model. \\n'\n",
      "                                   'I. Direct selling  refers to the '\n",
      "                                   'manufacturer or wholesaler of the '\n",
      "                                   'product/brand selling'},\n",
      "              'score': 0.603813767,\n",
      "              'values': []}],\n",
      " 'namespace': 'real',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Your query\n",
    "query = \"What is a business model?\"\n",
    "\n",
    "# Generate the embedding using the SentenceTransformer model\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# Assuming you have already initialized the Pinecone index\n",
    "# and created a `pinecone.Index` instance named `index`\n",
    "\n",
    "dres = index.query(vector=xq, top_k=2, namespace='real', include_metadata=True)\n",
    "\n",
    "# Print the results\n",
    "print(dres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (0.19.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (3.8.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sentence-transformers) (0.24.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from nltk->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from torchvision->sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "model/llama-2-7b-chat.ggmlv3.q4_0.bin is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/model/llama-2-7b-chat.ggmlv3.q4_0.bin/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1347\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1673\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    377\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    378\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    379\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    381\u001b[0m     )\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    399\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 400\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m     )\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-66bcd045-30e7761c5e4bf79a4573a37e;5761df7f-0790-4227-af88-ee6ee558739e)\n\nRepository Not Found for url: https://huggingface.co/model/llama-2-7b-chat.ggmlv3.q4_0.bin/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the embedding model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the local LLaMA-2 model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m llama_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/llama-2-7b-chat.ggmlv3.q4_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(llama_model_path)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize text generation pipeline\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:834\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    833\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 834\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    836\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:666\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    665\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 666\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages\\transformers\\utils\\hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: model/llama-2-7b-chat.ggmlv3.q4_0.bin is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import pinecone\n",
    "import time\n",
    "\n",
    "# Load the embedding model\n",
    "#embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the local LLaMA-2 model\n",
    "llama_model_path = \"model/llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_model_path)\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "\n",
    "limit = 3750\n",
    "\n",
    "def retrieve(query):\n",
    "    # Generate embeddings using the open-source model\n",
    "    xq = embedding_model.encode([query])[0]\n",
    "\n",
    "    # Retrieve from Pinecone\n",
    "    contexts = []\n",
    "    time_waited = 0\n",
    "    while (len(contexts) < 3 and time_waited < 60 * 12):\n",
    "        res = index.query(vector=xq.tolist(), top_k=3, namespace='real', include_metadata=True)\n",
    "        contexts = contexts + [\n",
    "            x['metadata']['text'] for x in res['matches']\n",
    "        ]\n",
    "        print(f\"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...\")\n",
    "        time.sleep(15)\n",
    "        time_waited += 15\n",
    "\n",
    "    if time_waited >= 60 * 12:\n",
    "        print(\"Timed out waiting for contexts to be retrieved.\")\n",
    "        contexts = [\"No contexts retrieved. Try to answer the question yourself!\"]\n",
    "\n",
    "    # Build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\" +\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # Append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts) - 1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "def complete(prompt):\n",
    "    # Generate completion using the local LLaMA-2 model\n",
    "    completion = generator(prompt, max_length=512, num_return_sequences=1)\n",
    "    return completion[0]['generated_text'].strip()\n",
    "\n",
    "# Example usage\n",
    "query = \"What is a Transformer?\"\n",
    "prompt = retrieve(query)\n",
    "answer = complete(prompt)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading llama_cpp_python-0.2.88.tar.gz (63.7 MB)\n",
      "     ---------------------------------------- 0.0/63.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/63.7 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/63.7 MB 393.8 kB/s eta 0:02:42\n",
      "     --------------------------------------- 0.1/63.7 MB 819.2 kB/s eta 0:01:18\n",
      "     ---------------------------------------- 0.2/63.7 MB 1.4 MB/s eta 0:00:47\n",
      "     ---------------------------------------- 0.5/63.7 MB 2.3 MB/s eta 0:00:28\n",
      "      --------------------------------------- 0.9/63.7 MB 3.7 MB/s eta 0:00:17\n",
      "     - -------------------------------------- 2.0/63.7 MB 6.6 MB/s eta 0:00:10\n",
      "     -- ------------------------------------- 3.2/63.7 MB 9.2 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 4.3/63.7 MB 10.9 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 5.5/63.7 MB 12.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 6.7/63.7 MB 13.7 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 8.1/63.7 MB 15.1 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 9.0/63.7 MB 15.6 MB/s eta 0:00:04\n",
      "     ------ -------------------------------- 10.3/63.7 MB 19.9 MB/s eta 0:00:03\n",
      "     ------- ------------------------------- 11.5/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     ------- ------------------------------- 12.7/63.7 MB 27.3 MB/s eta 0:00:02\n",
      "     -------- ------------------------------ 14.0/63.7 MB 27.3 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 15.1/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 15.6/63.7 MB 25.2 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 15.6/63.7 MB 25.2 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 18.8/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 19.8/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 21.1/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 22.3/63.7 MB 27.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 23.2/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 24.4/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     --------------- ----------------------- 25.7/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 26.8/63.7 MB 31.2 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 27.7/63.7 MB 27.3 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 29.1/63.7 MB 25.2 MB/s eta 0:00:02\n",
      "     ------------------ -------------------- 30.3/63.7 MB 25.2 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 31.3/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 32.7/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 33.9/63.7 MB 26.2 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 35.3/63.7 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 36.1/63.7 MB 27.3 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 37.7/63.7 MB 27.3 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 38.8/63.7 MB 28.4 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 40.2/63.7 MB 28.4 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 41.4/63.7 MB 28.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 42.7/63.7 MB 28.5 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 44.0/63.7 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 45.1/63.7 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 45.5/63.7 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 45.5/63.7 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 45.7/63.7 MB 20.5 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 47.7/63.7 MB 21.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 48.2/63.7 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 49.2/63.7 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 50.5/63.7 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 51.7/63.7 MB 20.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 52.6/63.7 MB 20.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 53.8/63.7 MB 19.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 55.1/63.7 MB 19.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 56.4/63.7 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 57.6/63.7 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 59.0/63.7 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 60.4/63.7 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 61.6/63.7 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  62.4/63.7 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  63.4/63.7 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  63.7/63.7 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  63.7/63.7 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  63.7/63.7 MB 25.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 63.7/63.7 MB 19.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\envs\\bot3\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.5/45.5 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.88-cp310-cp310-win_amd64.whl size=3055539 sha256=ca1738161c0b963280ca2ee8b0ce826068d7d239d24929fc7d198bbb8554979b\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\fc\\c5\\5a\\48a19d744c67b8886c5d644b8a01dff6c93fec6d34fa732349\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.88\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '13',\n",
      "              'metadata': {'text': 'Page | 4 \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   'jdj \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   ' \\n'\n",
      "                                   '4. C2B business model \\n'\n",
      "                                   'In this model, a consumer approaches a '\n",
      "                                   'website showing multiple business '\n",
      "                                   'organizations for a \\n'\n",
      "                                   'particular service. The consumer places an '\n",
      "                                   'estimate of amount he/she wants to spend '\n",
      "                                   'for a \\n'\n",
      "                                   'particular service.  \\n'\n",
      "                                   ' \\n'\n",
      "                                   'Examples \\n'\n",
      "                                   'Freelancer.com  \\n'\n",
      "                                   'Freelancer is an Australian crowd sourcing '\n",
      "                                   'marketplace website, which allows '\n",
      "                                   'potential \\n'\n",
      "                                   'employers to post jobs that freelancers '\n",
      "                                   'can then bid to complete. Where the '\n",
      "                                   'freelancers are'},\n",
      "              'score': 0.1967839,\n",
      "              'values': []},\n",
      "             {'id': '21',\n",
      "              'metadata': {'text': '7. C2G Model \\n'\n",
      "                                   'This model indicates the transactions '\n",
      "                                   'between government and consumers. Where '\n",
      "                                   'consumer \\n'\n",
      "                                   'provides any services or feedback that '\n",
      "                                   'adds value to the government '\n",
      "                                   'administration or \\n'\n",
      "                                   'authorities. It is an electronic platform '\n",
      "                                   'where consumers interact with the '\n",
      "                                   'government.  It \\n'\n",
      "                                   'covers the areas such as election, votes, '\n",
      "                                   'and taxation.   \\n'\n",
      "                                   'The C2G, short for Consumer to Government '\n",
      "                                   'E-commerce business allows consumers to '\n",
      "                                   'provide'},\n",
      "              'score': 0.161486313,\n",
      "              'values': []}],\n",
      " 'namespace': 'real',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Your query\n",
    "query = \"What is a Transformer?\"\n",
    "\n",
    "# Generate the embedding using the SentenceTransformer model\n",
    "xq = model.encode(query).tolist()\n",
    "\n",
    "# Assuming you have already initialized the Pinecone index\n",
    "# and created a `pinecone.Index` instance named `index`\n",
    "\n",
    "dres = index.query(vector=xq, top_k=2, namespace='real', include_metadata=True)\n",
    "\n",
    "# Print the results\n",
    "print(dres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is business \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the business model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m query_with_contexts \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m(query)\n\u001b[0;32m      7\u001b[0m query_with_contexts\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieve' is not defined"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is business \" +\n",
    "    \"in the business model\"\n",
    ")\n",
    "\n",
    "query_with_contexts = retrieve(query)\n",
    "query_with_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
